---
layout: default
title: Tutorial - Tree Species Classification
---
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75ede80c",
      "metadata": {
        "id": "75ede80c"
      },
      "source": [
        "# Deep learning in Tree species classification Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db97ddaf",
      "metadata": {
        "id": "db97ddaf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\" width=90%>\n",
        "  <td width=30% align=\"left\">     <a target=\"_blank\" href=\"https://colab.research.google.com/github/yuwei-cao-git/DRI-EDIA-F4A/blob/main/src/tree_species_classification/tree_species_classification.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">Run in Google Colab </a> </td>\n",
        "  <td width=30% align=\"left\">     <a target=\"_blank\" href=\"https://github.com/yuwei-cao-git/DRI-EDIA-F4A/blob/main/src/tree_species_classification/tree_species_classification.ipynb\"><img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">View on Github</a> </td>\n",
        "  <td width=30% align=\"left\">     <a href=\"https://drive.google.com/uc?id=1I8Lb3mAlkrUSSmdTyLQPQ52HhsGbF6qX\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">Download Data</a> </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed7b308",
      "metadata": {
        "id": "8ed7b308"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a3d46f9d",
      "metadata": {
        "id": "a3d46f9d"
      },
      "source": [
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b33b8d68",
      "metadata": {
        "id": "b33b8d68"
      },
      "source": [
        "‚öõ **Workflow**\n",
        "\n",
        "1. Set up the Dataset\n",
        "2. Create a model\n",
        "3. Train\n",
        "4. Test/Visualize result\n",
        "5. Tune the network\n",
        "6. Save/Depoly your model\n",
        "7. Scale up your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0cd5ec6",
      "metadata": {
        "id": "b0cd5ec6"
      },
      "source": [
        "# Install and load required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006c0526",
      "metadata": {
        "id": "006c0526"
      },
      "outputs": [],
      "source": [
        "#Uncomment this line to install packages\n",
        "# %pip install lightning gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5089e883",
      "metadata": {
        "id": "5089e883"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import lightning as L\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42621538",
      "metadata": {
        "id": "42621538"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e625fb2e",
      "metadata": {
        "id": "e625fb2e"
      },
      "outputs": [],
      "source": [
        "#Download the zipped tree crown data\n",
        "!gdown 1svN8wVUmgvyQeOgj_NZkQtp7m7ehUEu2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd283fb4",
      "metadata": {
        "id": "fd283fb4"
      },
      "outputs": [],
      "source": [
        "#Remove data dir if it already exists\n",
        "if os.path.exists(\"data\"):\n",
        "    shutil.rmtree(\"data\")\n",
        "\n",
        "#Unzip the data\n",
        "!unzip qc_crowns.zip -d data/\n",
        "\n",
        "#Remove zip file\n",
        "!rm qc_crowns.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766d4bda",
      "metadata": {
        "id": "766d4bda"
      },
      "outputs": [],
      "source": [
        "# List files in the current directory\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ab87734",
      "metadata": {
        "id": "4ab87734"
      },
      "source": [
        "# Load Crown Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ab2931",
      "metadata": {
        "id": "02ab2931"
      },
      "outputs": [],
      "source": [
        "#Load the crown polygons\n",
        "crowns_df = gpd.read_file('data/tree_crowns_subset.gpkg')\n",
        "\n",
        "# Map class labels to binary values\n",
        "label_mapping = {'coniferous': 0, 'deciduous': 1}\n",
        "crowns_df['label'] = crowns_df['species_type'].map(label_mapping)\n",
        "\n",
        "#Set data dir\n",
        "img_dir = 'data/clipped_crowns'\n",
        "img_fpaths = list(Path(img_dir).glob(\"*.png\"))\n",
        "\n",
        "#Convert fpaths ls to data frame\n",
        "img_df = pd.DataFrame(img_fpaths, columns=['fpath'])\n",
        "img_df['crown_id'] = img_df['fpath'].apply(lambda x: int(x.stem.split(\".\")[0].split(\"_\")[1]))\n",
        "\n",
        "#Join with crowns_df\n",
        "crowns_df = crowns_df.merge(img_df, on='crown_id', how='left')\n",
        "crowns_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e5f4dc",
      "metadata": {
        "id": "64e5f4dc"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the count plot with 'label'\n",
        "ax = sns.countplot(data=crowns_df, x='label', hue='label', palette='viridis', legend=False)\n",
        "\n",
        "# Add a custom legend\n",
        "legend_labels = {0: 'Coniferous', 1: 'Deciduous'}\n",
        "handles = [plt.Rectangle((0, 0), 1, 1, color=ax.patches[i].get_facecolor()) for i in range(len(legend_labels))]\n",
        "plt.legend(handles, legend_labels.values(), title=\"Tree Type\")\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Labels')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b9e3f9",
      "metadata": {
        "id": "28b9e3f9"
      },
      "source": [
        "# Set up Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4afb83d",
      "metadata": {
        "id": "b4afb83d"
      },
      "outputs": [],
      "source": [
        "class TreeCrownDataset(Dataset):\n",
        "    def __init__(self, crowns_df, split, target_res=256, train_augmentations=[]):\n",
        "        self.target_res = target_res\n",
        "        self.split = split\n",
        "        self.crowns_df = crowns_df\n",
        "        self.train_augmentations = train_augmentations\n",
        "\n",
        "        # Create a transform to resize and normalize the crown images\n",
        "        self.transforms = [\n",
        "            transforms.Resize((target_res, target_res)),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "\n",
        "        #Add additional transforms for data augmentation if using train dataset\n",
        "        if self.split == 'train':\n",
        "            self.transforms.extend(self.train_augmentations)\n",
        "\n",
        "        # Build transform pipeline\n",
        "        self.transforms = transforms.Compose(self.transforms)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.crowns_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        target_crown = self.crowns_df.iloc[idx]\n",
        "\n",
        "        label = torch.tensor(target_crown['label']).long()\n",
        "\n",
        "        crown_img = Image.open(target_crown['fpath']).convert('RGB')\n",
        "\n",
        "        crown_tensor = self.transforms(crown_img)\n",
        "\n",
        "        crown_id = target_crown['crown_id']\n",
        "\n",
        "        return crown_tensor, label, crown_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4f808f",
      "metadata": {
        "id": "1f4f808f"
      },
      "source": [
        "# Set up the Lightning Data Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a43de2",
      "metadata": {
        "id": "79a43de2"
      },
      "outputs": [],
      "source": [
        "class TreeCrownDataModule(L.LightningDataModule):\n",
        "    def __init__(self, crowns_df, batch_size=32, train_augmentations=[]):\n",
        "        super().__init__()\n",
        "        self.crowns_df = crowns_df\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        #Split data into three dataframes for train/val/test\n",
        "        train_val_df, self.test_df = train_test_split(self.crowns_df,\n",
        "                                                      test_size=0.15,\n",
        "                                                      random_state=42)\n",
        "\n",
        "        self.train_df, self.val_df = train_test_split(train_val_df,\n",
        "                                                      test_size=0.17,\n",
        "                                                      random_state=42)\n",
        "\n",
        "        #Report dataset sizes\n",
        "        for name, df in [(\"Train\", self.train_df),\n",
        "                         (\"Val\", self.val_df),\n",
        "                         (\"Test\", self.test_df)]:\n",
        "\n",
        "            print(f\"{name} dataset size: {len(df)}\",\n",
        "                  f\"({round(len(df)/len(crowns_df)*100, 0)}%)\")\n",
        "\n",
        "        # Instantiate datasets\n",
        "        self.train_dataset = TreeCrownDataset(self.train_df, split='train')\n",
        "\n",
        "        self.val_dataset = TreeCrownDataset(self.val_df, split='val')\n",
        "\n",
        "        self.test_dataset = TreeCrownDataset(self.test_df, split='test')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.test_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False\n",
        "                          )\n",
        "\n",
        "#Set the training data augmentations\n",
        "train_augmentations = [\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation([-90, 90]),\n",
        "                transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0))\n",
        "                ]\n",
        "\n",
        "# Test the datamodule\n",
        "crowns_datamodule = TreeCrownDataModule(crowns_df, train_augmentations=train_augmentations)\n",
        "crowns_datamodule.setup()\n",
        "\n",
        "# Test loading a sample\n",
        "sample = crowns_datamodule.train_dataset[0]\n",
        "print(sample[0].shape)\n",
        "print(sample[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0324098",
      "metadata": {
        "id": "d0324098"
      },
      "source": [
        "# Set up The Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8062e204",
      "metadata": {
        "id": "8062e204"
      },
      "outputs": [],
      "source": [
        "class CNN(L.LightningModule):\n",
        "    def __init__(self, lr, pretrained_weights=True):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT if pretrained_weights else None) # IMAGENET1K_V2 vs. random init\n",
        "\n",
        "        # Modify the final fc layer of model to output a single value for binary classification\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, 1)\n",
        "\n",
        "        #Add sigmoid activation to the end model\n",
        "        self.model = nn.Sequential(self.model, nn.Sigmoid())\n",
        "\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "        self.lr = lr\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch\n",
        "        y_hat = self(x).squeeze()\n",
        "        loss = self.criterion(y_hat, y.float())\n",
        "        self.log('train_loss', loss, on_epoch=True, on_step=False)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch\n",
        "        y_hat = self(x).squeeze()\n",
        "        loss = self.criterion(y_hat, y.float())\n",
        "        self.log('val_loss', loss, on_epoch=True, on_step=False)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, y, id = batch\n",
        "        y_hat = self(x).squeeze()\n",
        "\n",
        "        return y_hat, y, id\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a0e82d",
      "metadata": {
        "id": "a1a0e82d"
      },
      "outputs": [],
      "source": [
        "#Instantiate the model with 1 class (present/absent)\n",
        "model = CNN(lr=0.0001)\n",
        "print(model)\n",
        "\n",
        "#Try passing some data through the model\n",
        "batch, labels, ids = next(iter(crowns_datamodule.train_dataloader()))\n",
        "\n",
        "# Pass batch through the model\n",
        "y_hat = model(batch)\n",
        "\n",
        "print(\"\\nCrown IDs:\\n\", ids)\n",
        "\n",
        "print(\"\\nImage batch shape:\\n\", batch.shape)\n",
        "\n",
        "print(\"\\nOutput tensor shape:\\n\", y_hat.shape)\n",
        "\n",
        "#View the predicted class probabilities\n",
        "print(\"\\nPredicted class probabilities:\\n\",\n",
        "      y_hat.detach().cpu().numpy().squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53c84841",
      "metadata": {
        "id": "53c84841"
      },
      "source": [
        "# Set up Lightning Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2770ff",
      "metadata": {
        "id": "1f2770ff"
      },
      "outputs": [],
      "source": [
        "# put together\n",
        "crowns_datamodule = TreeCrownDataModule(crowns_df, train_augmentations=[])\n",
        "crowns_datamodule.setup()\n",
        "model = CNN(lr=0.0001)\n",
        "tensorboard_logger = TensorBoardLogger('', name='lightning_logs', version=0)\n",
        "trainer = L.Trainer(max_epochs=10, logger=[tensorboard_logger], devices=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8280dc1",
      "metadata": {
        "id": "a8280dc1"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca1a084",
      "metadata": {
        "id": "aca1a084"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, datamodule=crowns_datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad39173",
      "metadata": {
        "id": "aad39173"
      },
      "source": [
        "# Visualize learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45529e3",
      "metadata": {
        "id": "f45529e3"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35737fe6",
      "metadata": {
        "id": "35737fe6"
      },
      "outputs": [],
      "source": [
        "def calc_test_oa():\n",
        "    #Test the model on the test set\n",
        "    out = trainer.predict(model, datamodule=crowns_datamodule, return_predictions=True)\n",
        "\n",
        "    # Separate predictions and targets from output\n",
        "    pred_class_probs = np.concatenate([batch[0] for batch in out])\n",
        "    obs = np.concatenate([batch[1] for batch in out])\n",
        "    ids = np.concatenate([batch[2] for batch in out])\n",
        "\n",
        "    #Convert to obs-pred dataframe\n",
        "    test_df = pd.DataFrame({'obs': obs, 'pred_class_probs': pred_class_probs, 'crown_id': ids})\n",
        "\n",
        "    #Convert class probabilities to binary predictions\n",
        "    test_df['pred_boolean_class'] = (test_df['pred_class_probs'] > 0.5)\n",
        "\n",
        "    #Convert binary predictions to integers\n",
        "    test_df['pred'] = test_df['pred_boolean_class'].astype(int)\n",
        "\n",
        "    #Add a column for correct/incorrect predictions\n",
        "    test_df['correct'] = test_df['obs'] == test_df['pred']\n",
        "\n",
        "    #Join with crowns_df\n",
        "    test_df = test_df.merge(crowns_df, on='crown_id', how='left')\n",
        "\n",
        "    #Calculate overall accuracy using sklearn\n",
        "    overall_acc = sklearn.metrics.accuracy_score(y_true=test_df['obs'], y_pred=test_df['pred'])\n",
        "\n",
        "\n",
        "    #Check how many crowns were classified correctly\n",
        "    n_correct = len(test_df[test_df['correct'] == True])\n",
        "\n",
        "    print(f\"Summary: {n_correct} / {len(test_df)} crowns were classified correctly.\")\n",
        "    return overall_acc, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0984c04",
      "metadata": {
        "id": "f0984c04"
      },
      "outputs": [],
      "source": [
        "overall_acc, test_df = calc_test_oa()\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2230dbff",
      "metadata": {
        "id": "2230dbff"
      },
      "outputs": [],
      "source": [
        "print(label_mapping)\n",
        "\n",
        "#Generate a confusion matrix using seaborn\n",
        "cm = confusion_matrix(y_true=test_df['obs'],\n",
        "                      y_pred=test_df['pred'])\n",
        "\n",
        "#Plot the confusion matrix\n",
        "classes = ['Coniferous', 'Deciduous']\n",
        "sns.heatmap(cm, annot=True,\n",
        "            cmap='YlGn',\n",
        "            xticklabels=classes,\n",
        "            yticklabels=classes)\n",
        "\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Observed')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa86c18f",
      "metadata": {
        "id": "fa86c18f"
      },
      "outputs": [],
      "source": [
        "# Let's view the incorrectly classified crowns\n",
        "incorrect_df = test_df[test_df['correct'] == False]\n",
        "\n",
        "#Plot incorrecty classified coniferous/deciduous crowns\n",
        "\n",
        "for c_type in test_df['species_type'].unique():\n",
        "\n",
        "    print(f\"\\nIncorrectly classified {c_type} crowns.\\n\")\n",
        "\n",
        "    # Filter the incorrect crowns by species type\n",
        "    incorrect_type_df = test_df[(test_df['correct'] == False) & (test_df['species_type'] == c_type)]\n",
        "\n",
        "    # Number of images\n",
        "    num_images = len(incorrect_type_df)\n",
        "\n",
        "    # Determine the grid size\n",
        "    grid_size = int(num_images**0.5) + 1\n",
        "\n",
        "    # Create a figure and axes\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
        "\n",
        "    # Flatten the axes array for easy iteration\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Read the incorrect crown files and plot them\n",
        "    for ax, fpath in zip(axes, incorrect_type_df['fpath']):\n",
        "        img = Image.open(fpath)\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any remaining empty subplots\n",
        "    for ax in axes[num_images:]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96ff689",
      "metadata": {
        "id": "e96ff689"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814d7545",
      "metadata": {
        "id": "814d7545"
      },
      "source": [
        "# Tune hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd38087",
      "metadata": {
        "id": "fcd38087"
      },
      "source": [
        "Forget about ML for a second. Imagine you are baking a cookie. You have 3 things you can change about the cookie:\n",
        "\n",
        "- Sugar type (white, brown, cane)\n",
        "- Baking time (15 minutes, 30 minutes)\n",
        "- Cooking temperature (360, 400 degrees)\n",
        "\n",
        "There are 12 possible variations of cookies you can make. One of them will be the most delicious.\n",
        "\n",
        "To find out which cookie tastes the best, you need to make all variations and assign a score\n",
        "- ü§¢\n",
        "- ü§î\n",
        "- üòÜ\n",
        "- üòç\n",
        "\n",
        "This is called a hyperparameter sweep. Your three hyperparameters are sugar, baking time, cooking temperature.\n",
        "\n",
        "```\n",
        "python make_cookie.py --sugar 'white' --baking_time 15 --temperature 400\n",
        "python make_cookie.py --sugar 'brown' --baking_time 15 --temperature 400\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0eac6cc",
      "metadata": {
        "id": "a0eac6cc"
      },
      "source": [
        "üèÑüèΩ‚Äç‚ôÄÔ∏è **what combination of parameters produces the best performing model?**\n",
        "\n",
        "The definition of \"best\" depends on the work you are doing. In general, \"best\" refers to the lowest loss. At Lightning, we tend to think of \"best\" as the lowest loss for the least amount of time spent training.\n",
        "\n",
        "If we run this training script with different hyperparameter combinations, it produces different loss curves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4631ce31",
      "metadata": {
        "id": "4631ce31"
      },
      "source": [
        "##### test 1: pretrained weigths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5da4ace",
      "metadata": {
        "id": "a5da4ace"
      },
      "outputs": [],
      "source": [
        "# put together\n",
        "crowns_datamodule = TreeCrownDataModule(crowns_df, train_augmentations=[])\n",
        "crowns_datamodule.setup()\n",
        "csv_logger = CSVLogger('', name='logs', version=1)\n",
        "tensorboard_logger = TensorBoardLogger('', name='lightning_logs', version=1)\n",
        "model = CNN(lr=0.01, pretrained_weights=False)\n",
        "trainer = L.Trainer(max_epochs=10, logger=[csv_logger, tensorboard_logger], devices=1)\n",
        "trainer.fit(model, datamodule=crowns_datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1617c78d",
      "metadata": {
        "id": "1617c78d"
      },
      "outputs": [],
      "source": [
        "overall_acc, test_df = calc_test_oa()\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf9be60",
      "metadata": {
        "id": "fdf9be60"
      },
      "source": [
        "##### test 2: different learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30e4ae4",
      "metadata": {
        "id": "b30e4ae4"
      },
      "outputs": [],
      "source": [
        "# put together\n",
        "model = CNN(lr=0.01)\n",
        "csv_logger = CSVLogger('', name='logs', version=2)\n",
        "tensorboard_logger = TensorBoardLogger('', name='lightning_logs', version=2)\n",
        "trainer = L.Trainer(max_epochs=10, logger=[csv_logger, tensorboard_logger], devices=1)\n",
        "trainer.fit(model, datamodule=crowns_datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fb3b50",
      "metadata": {
        "id": "b0fb3b50"
      },
      "outputs": [],
      "source": [
        "overall_acc, test_df = calc_test_oa()\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3624c9d",
      "metadata": {
        "id": "b3624c9d"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4c54d3",
      "metadata": {
        "id": "0d4c54d3"
      },
      "source": [
        "# Save/Depoly your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3177e55",
      "metadata": {
        "id": "c3177e55"
      },
      "outputs": [],
      "source": [
        "trainer.save_checkpoint(filepath=\".ckpt/model.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19534870",
      "metadata": {
        "id": "19534870"
      },
      "outputs": [],
      "source": [
        "model = CNN.load_from_checkpoint(\".ckpt/model.ckpt\", lr=0.01)\n",
        "model.freeze()\n",
        "\n",
        "crowns_datamodule = TreeCrownDataModule(crowns_df, train_augmentations=[])\n",
        "crowns_datamodule.setup()\n",
        "test_predictions = trainer.predict(model, datamodule=crowns_datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff50f8b",
      "metadata": {
        "id": "1ff50f8b"
      },
      "outputs": [],
      "source": [
        "overall_acc, test_df = calc_test_oa()\n",
        "print(f\"Overall accuracy: {overall_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735fec2",
      "metadata": {
        "id": "5735fec2"
      },
      "source": [
        "TorchScript allows you to serialize your models in a way that it can be loaded in non-Python environments. The LightningModule has a handy method to_torchscript() that returns a scripted module which you can save or directly use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b02319a",
      "metadata": {
        "id": "5b02319a"
      },
      "outputs": [],
      "source": [
        "script = model.to_torchscript()\n",
        "\n",
        "# save for use in production environment\n",
        "torch.jit.save(script, \".ckpt/model.pt\")\n",
        "\n",
        "# use it\n",
        "#Try passing some data through the model\n",
        "batch, labels, ids = next(iter(crowns_datamodule.test_dataloader()))\n",
        "\n",
        "scripted_module = torch.jit.load(\".ckpt/model.pt\")\n",
        "output = scripted_module(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43774a2d",
      "metadata": {
        "id": "43774a2d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b932292e",
      "metadata": {
        "id": "b932292e"
      },
      "source": [
        "# Scale up your model/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05d58d4",
      "metadata": {
        "id": "e05d58d4"
      },
      "source": [
        "You can either make all cookies sequentially (which will take you 4.5 hours). Or you can get 12 kitchens and cook them all in parallel, and you'll know in 30 minutes.\n",
        "\n",
        "If a kitchen is a GPU, then you need 12 GPUs to run each experiment to see which cookie is the best. The power of Lightning is the ability to run sweeps like this on 12 different GPUs (or 1,000 GPUs if you'd like) to get you the best version of a model fast."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e845a55d",
      "metadata": {
        "id": "e845a55d"
      },
      "source": [
        "Train on GPUs\n",
        "The Trainer will run on all available GPUs by default. Make sure you‚Äôre running on a machine with at least one GPU. There‚Äôs no need to specify any NVIDIA flags as Lightning will do it for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d04ff0c",
      "metadata": {
        "id": "0d04ff0c"
      },
      "outputs": [],
      "source": [
        "from lightning import Trainer\n",
        "\n",
        "# run on as many GPUs as available by default\n",
        "trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n",
        "# equivalent to\n",
        "trainer = Trainer()\n",
        "\n",
        "# run on one GPU\n",
        "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
        "# run on multiple GPUs\n",
        "trainer = Trainer(accelerator=\"gpu\", devices=8)\n",
        "# choose the number of devices automatically\n",
        "trainer = Trainer(accelerator=\"gpu\", devices=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0df2fc",
      "metadata": {
        "id": "0f0df2fc"
      },
      "source": [
        "Train on Slurm Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1331b0d9",
      "metadata": {
        "id": "1331b0d9"
      },
      "outputs": [],
      "source": [
        "# train.py\n",
        "def main(args):\n",
        "    model = CNN(args)\n",
        "\n",
        "    trainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"ddp\")\n",
        "\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = ...  # you can use your CLI parser of choice, or the `LightningCLI` or using config.yaml\n",
        "    # TRAIN\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b06940",
      "metadata": {
        "id": "17b06940"
      },
      "outputs": [],
      "source": [
        "%%writefile submit.sh\n",
        "# (submit.sh)\n",
        "#!/bin/bash -l\n",
        "\n",
        "# SLURM SUBMIT SCRIPT\n",
        "#SBATCH --nodes=4             # This needs to match Trainer(num_nodes=...)\n",
        "#SBATCH --gres=gpu:8\n",
        "#SBATCH --ntasks-per-node=8   # This needs to match Trainer(devices=...)\n",
        "#SBATCH --mem=0\n",
        "#SBATCH --time=0-02:00:00\n",
        "\n",
        "# activate conda env\n",
        "source activate $1\n",
        "\n",
        "# debugging flags (optional)\n",
        "export NCCL_DEBUG=INFO\n",
        "export PYTHONFAULTHANDLER=1\n",
        "\n",
        "# on your cluster you might need these:\n",
        "# set the network interface\n",
        "# export NCCL_SOCKET_IFNAME=^docker0,lo\n",
        "\n",
        "# might need the latest CUDA\n",
        "# module load NCCL/2.4.7-1-cuda.10.0\n",
        "\n",
        "# run script from above\n",
        "srun python3 train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4441aa1",
      "metadata": {
        "id": "a4441aa1"
      },
      "outputs": [],
      "source": [
        "%%!\n",
        "sbatch submit.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8fd6608",
      "metadata": {
        "id": "b8fd6608"
      },
      "source": [
        "Or you can even parallel the baking procedure..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b3dd0b",
      "metadata": {
        "id": "77b3dd0b"
      },
      "source": [
        "![image](https://shared.fastly.steamstatic.com/store_item_assets/steam/apps/448510/ss_2a35c15c78f06dd4f23dab8a1e1917a835d0062d.1920x1080.jpg?t=1741368176)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d117b3",
      "metadata": {
        "id": "57d117b3"
      },
      "source": [
        "# wandb sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9303ada5",
      "metadata": {
        "id": "9303ada5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d307c740",
      "metadata": {
        "id": "d307c740"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://api.wandb.ai/links/ubc-yuwei-cao/ebnspmv1\" style=\"border:none;height:1024px;width:100%\">"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
